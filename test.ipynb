{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Open the existing document\n",
    "doc = Document(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\result1.docx\")\n",
    "\n",
    "# # Add text at the start of the document\n",
    "# doc.paragraphs.insert(0, doc.add_paragraph('Text at the start'))\n",
    "\n",
    "# Add text at the end of the document\n",
    "doc.add_paragraph(\"whole_word_document_end\")\n",
    "\n",
    "# Save the document\n",
    "doc.save(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\result1_add.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'insert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(soup))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43madd_text_to_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m2113196\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmerged.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<p>This is the new starting text.</p>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<p>This is the new ending text.</p>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m, in \u001b[0;36madd_text_to_html\u001b[1;34m(filename, start_text, end_text)\u001b[0m\n\u001b[0;32m      6\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add text at the start of the body\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m(\u001b[38;5;241m0\u001b[39m, BeautifulSoup(start_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Add text at the end of the body\u001b[39;00m\n\u001b[0;32m     12\u001b[0m soup\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mappend(BeautifulSoup(end_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'insert'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def add_text_to_html(filename, start_text, end_text):\n",
    "    # Load the HTML document\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    # Add text at the start of the body\n",
    "    soup.body.insert(0, BeautifulSoup(start_text, 'html.parser'))\n",
    "\n",
    "    # Add text at the end of the body\n",
    "    soup.body.append(BeautifulSoup(end_text, 'html.parser'))\n",
    "\n",
    "    # Save the modified document\n",
    "    with open(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\merged1.html\", 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(str(soup))\n",
    "\n",
    "# Usage\n",
    "add_text_to_html(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\merged.html\", '<p>This is the new starting text.</p>', '<p>This is the new ending text.</p>')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fileinput\n",
    "\n",
    "# specify the directory containing the HTML files to merge\n",
    "html_dir = \"C:\\\\Users\\\\2113196\\\\Downloads\\\\html_split\\\\\"\n",
    "\n",
    "# create a list of all HTML files in the directory\n",
    "html_files = [f for f in os.listdir(html_dir) if f.endswith('.html')]\n",
    "\n",
    "# sort the list of HTML files to ensure proper merging order\n",
    "html_files.sort()\n",
    "\n",
    "# create a new HTML file to merge the contents into\n",
    "with open(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\merged.html\", 'w', encoding=\"utf-8\") as outfile:\n",
    "    # iterate over each HTML file in the directory\n",
    "    for html_file in html_files:\n",
    "        # construct the full path to the HTML file\n",
    "        html_path = os.path.join(html_dir, html_file)\n",
    "        \n",
    "        # iterate over each line in the HTML file\n",
    "        for line in open(html_path, encoding=\"utf-8\"):\n",
    "            # write the line to the output file\n",
    "            outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting htmlmerger\n",
      "  Downloading htmlmerger-0.1.46.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: htmlmerger\n",
      "  Running setup.py install for htmlmerger: started\n",
      "  Running setup.py install for htmlmerger: finished with status 'done'\n",
      "Successfully installed htmlmerger-0.1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: htmlmerger is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install htmlmerger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if start_tag == \"Literature\":\n",
    "        #     occurrence = 11\n",
    "        \n",
    "        if start_tag == \"Introduction\":\n",
    "            end_tag = \"Worldwide marketing\"\n",
    "        if start_tag == \"Worldwide marketing authorization status\":\n",
    "            occurrence = 2\n",
    "            start_tag = \"authorization status\"\n",
    "            end_tag = \"taken in the reporting interval\"          \n",
    "        if start_tag == \"Actions taken in the reporting interval for safety reasons\":\n",
    "            start_tag = \"taken in the reporting interval\"\n",
    "            end_tag = \"Changes to reference\"\n",
    "        if start_tag == \"Changes to reference safety information\":\n",
    "            start_tag = \"Changes to reference\"\n",
    "            end_tag = \"Estimated exposure and use\"\n",
    "        if start_tag == \"Estimated exposure and use patterns\":\n",
    "            start_tag = \"Estimated exposure and use\"\n",
    "            end_tag = \"Data in summary tabulations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def merge_documents(files):\n",
    "    merged_document = Document()\n",
    "\n",
    "    for file in files:\n",
    "        sub_doc = Document(file)\n",
    "\n",
    "        # Loop through all the paragraph, table and picture in the sub document\n",
    "        for element in sub_doc.element.body:\n",
    "            merged_document.element.body.append(element)\n",
    "\n",
    "    # Save the merged document\n",
    "    merged_document.save('C:\\\\Users\\\\2113196\\\\Downloads\\\\split\\\\merged.docx')\n",
    "\n",
    "# List of files to merge\n",
    "files = ['C:\\\\Users\\\\2113196\\\\Downloads\\\\split\\\\section 4.docx', 'C:\\\\Users\\\\2113196\\\\Downloads\\\\split\\\\section 5.docx']\n",
    "\n",
    "# Call the function\n",
    "merge_documents(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 2\n",
    "    # start_tag = 'Worldwide marketing authorization status'\n",
    "    # end_tag = 'Actions taken in the reporting interval for safety reasons'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 2\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 3\n",
    "    # start_tag = 'Actions taken in the reporting interval for safety reasons'\n",
    "    # end_tag = 'Changes to reference safety information'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 3\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 4\n",
    "    # start_tag = 'Changes to reference safety information'\n",
    "    # end_tag = 'Estimated exposure and use patterns'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 4\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 5\n",
    "    # start_tag = 'Estimated exposure and use patterns'\n",
    "    # end_tag = 'Data in summary tabulations'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 5\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 6\n",
    "    # start_tag = 'Data in summary tabulations'\n",
    "    # end_tag = 'Summaries of significant findings from clinical trials in the reporting interval'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 6\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 7\n",
    "    # start_tag = 'Summaries of significant findings from clinical trials in the reporting interval'\n",
    "    # end_tag = 'Findings from non-interventional studies'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 7\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 8\n",
    "    # start_tag = 'Findings from non-interventional studies'\n",
    "    # end_tag = 'Information from other clinical trials and sources'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 8\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 9\n",
    "    # start_tag = 'Information from other clinical trials and sources'\n",
    "    # end_tag = 'Non-clinical data'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 9\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 10\n",
    "    # start_tag = 'Non-clinical data'\n",
    "    # end_tag = 'Literature'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 10\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 11\n",
    "    # start_tag = 'Akash'\n",
    "    # end_tag = 'Akash kumar'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 11\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 12\n",
    "    # start_tag = 'Other periodic reports'\n",
    "    # end_tag = 'Lack of efficacy in controlled clinical trials'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 12\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 13\n",
    "    # start_tag = 'Lack of efficacy in controlled clinical trials'\n",
    "    # end_tag = 'Late-breaking information'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 13\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 14\n",
    "    # start_tag = 'Late-breaking information'\n",
    "    # end_tag = 'Overview of signals: new, ongoing and closed'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 14\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 15\n",
    "    # start_tag = 'Overview of signals: new, ongoing and closed'\n",
    "    # end_tag = 'Signal and risk evaluation'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 15\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 16\n",
    "    # start_tag = 'Signal and risk evaluation'\n",
    "    # end_tag = 'Benefit evaluation'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 16\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 17\n",
    "    # start_tag = 'Benefit evaluation'\n",
    "    # end_tag = 'Integrated benefit/risk analysis for authorized indications'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 17\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 18\n",
    "    # start_tag = 'Integrated benefit/risk analysis for authorized indications'\n",
    "    # end_tag = 'Conclusions and actions'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 18\"\n",
    "    # run_xml_to_docs(section_name)\n",
    "\n",
    "    # # Section 19\n",
    "    # start_tag = 'Conclusions and actions'\n",
    "    # end_tag = 'References'\n",
    "    # split_document_section_wise(start_tag, end_tag)\n",
    "    # section_name = \"section 19\"\n",
    "    # run_xml_to_docs(section_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import copy\n",
    "\n",
    "# Open the Word document containing the table\n",
    "src_doc = docx.Document(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\aka.docx\")\n",
    "\n",
    "# Get the table object to be copied\n",
    "table = src_doc.tables[0]\n",
    "\n",
    "# Create a deep copy of the table object\n",
    "copied_table = copy.deepcopy(table)\n",
    "\n",
    "# Open the Word document where the copied table will be inserted\n",
    "dest_doc = docx.Document(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\result1.docx\")\n",
    "\n",
    "# Insert the copied table into the destination Word document\n",
    "dest_doc.add_table(copied_table._tbl,1)\n",
    "\n",
    "# Save the destination Word document\n",
    "dest_doc.save(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\result1.docx\")\n",
    "\n",
    "# Print the copied table to the console\n",
    "for row in copied_table.rows:\n",
    "    for cell in row.cells:\n",
    "        print(cell.text, end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import docx\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\intestinal_performation_case_line_listing_table_cumulative.xlsx\")\n",
    "\n",
    "# Create a new Word document or open an existing one\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a table to the document and create a reference variable\n",
    "# Extra row is for the header row\n",
    "t = doc.add_table(df.shape[0]+1, df.shape[1])\n",
    "t.style = 'Table Grid'\n",
    "\n",
    "# Add the header rows\n",
    "for j in range(df.shape[-1]):\n",
    "    t.cell(0, j).text = str(df.columns[j])\n",
    "\n",
    "# Add the rest of the DataFrame\n",
    "for i in range(df.shape[0]):\n",
    "    for j in range(df.shape[-1]):\n",
    "        t.cell(i+1, j).text = str(df.values[i, j])\n",
    "\n",
    "# Save the document\n",
    "doc.save(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\aaaaaaaaaaa.docx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from lxml import etree\n",
    "def xml_to_docx(xml_content, output_path):\n",
    "    tmp_dir = \"C:\\\\Users\\\\2113196\\\\Downloads\\\\Table5_1\"\n",
    "    xml_path = os.path.join(tmp_dir, 'word\\\\document.xml')\n",
    "    with open(xml_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(xml_content)\n",
    "    with zipfile.ZipFile(output_path, 'w') as zip_ref:\n",
    "        for root, dirs, files in os.walk(tmp_dir):\n",
    "            for file in files:\n",
    "                zip_ref.write(os.path.join(root, file),\n",
    "                              os.path.relpath(os.path.join(root, file), tmp_dir))\n",
    "    # shutil.rmtree(tmp_dir)\n",
    "\n",
    "tmp_dir = \"C:\\\\Users\\\\2113196\\\\Downloads\\\\Table5_1\"\n",
    "xml_path = os.path.join(tmp_dir, 'word\\\\document.xml')\n",
    "with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "    xml_content = file.read()\n",
    "# Modify the XML content as needed\n",
    "# For example, you can parse it with lxml and modify it\n",
    "root = etree.fromstring(xml_content.encode('utf-8'))\n",
    "# Modify the root element or its children as needed\n",
    "\n",
    "# Convert the modified XML back to a Word document\n",
    "xml_to_docx(etree.tostring(root, pretty_print=True).decode('utf-8'), \"C:\\\\Users\\\\2113196\\\\Downloads\\\\output_docx_file.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "def extract_xml_from_docx(docx_path):\n",
    "    tmp_dir = \"C:\\\\Users\\\\2113196\\\\Downloads\\\\Table5_1\"\n",
    "    with zipfile.ZipFile(docx_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(tmp_dir)\n",
    "    xml_path = os.path.join(tmp_dir, 'word/document.xml')\n",
    "    with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "    return xml_content, tmp_dir\n",
    "\n",
    "# Extract XML from a Word document\n",
    "# tmp_dir = \"C:\\\\Users\\\\2113196\\\\Downloads\\\\new\"\n",
    "# xml_path = os.path.join(tmp_dir, 'word/document.xml')\n",
    "# print(xml_path)\n",
    "# with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "#     xml_content = file.read()\n",
    "\n",
    "xml_content, tmp_dir = extract_xml_from_docx(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\Table5_1.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data for the table\n",
    "data = {\n",
    "    'Formulation': ['a','b'],\n",
    "    'Current reporting interval 01 Oct 2013 to 30 Sep 2015 01 Oct 2013 to 30 Sep 2015 Amount sold (mg)':0,\n",
    "    'Estimated exposure (patient-years)':0,\n",
    "    'Cumulative Until 30 Sep 2015 Until 30 Sep 2015 Amount sold (mg)':0,\n",
    "    'Estimated exposure (patient-years) ':0,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spire.xls import *\n",
    "from spire.xls.common import *\n",
    "\n",
    "# Instantiate a Workbook object\n",
    "workbook = Workbook()\n",
    "# Load an Excel file\n",
    "workbook.LoadFromFile(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\LineListingTable.xlsx\")\n",
    "\n",
    "# Save the Excel file to HTML format\n",
    "workbook.SaveToHtml(\"C:\\\\Users\\\\2113196\\\\Downloads\\\\ExcelToHTML.html\")\n",
    "\n",
    "workbook.Dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel to DataFrame\n",
    "path_excel = 'C:\\\\Users\\\\2113196\\\\Downloads\\\\clinical_table.xlsx'\n",
    "df = pd.read_excel(path_excel, engine='openpyxl')\n",
    "\n",
    "# Convert DataFrame to JSON\n",
    "json_data = df.to_json(orient='records', indent=4)\n",
    "print(json_data)\n",
    "\n",
    "# Write JSON data to file (optional)\n",
    "# path_json = 'final_result.json'\n",
    "# with open(path_json, 'w') as json_file:\n",
    "#     json_file.write(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
